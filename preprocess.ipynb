{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import needed library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/16 15:37:55 WARN Utils: Your hostname, fyjdeMacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.20.167.241 instead (on interface en0)\n",
      "22/12/16 15:37:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/fyj/opt/anaconda3/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/12/16 15:37:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SQLContext, SparkContext\n",
    "from pyspark.sql.functions import mean, col, udf\n",
    "from pyspark.sql.types import *\n",
    "import numpy as np\n",
    "import pandas\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "spark_context = SparkContext('local')\n",
    "sqlc = SQLContext(spark_context)\n",
    "train = sqlc.read.csv('data/train.csv', sep='\\t', header=True)\n",
    "test = sqlc.read.csv('data/test.csv', sep='\\t', header=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Problem Hieararchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def get_unit(str):\n",
    "    return str.split(\",\")[0]\n",
    "\n",
    "@udf\n",
    "def get_section(str):\n",
    "     return str.split(\",\")[1]\n",
    "\n",
    "train = train.withColumn(\"Problem Unit\",get_unit(train[\"Problem Hierarchy\"]))\n",
    "train = train.withColumn(\"Problem Section\",get_section(train[\"Problem Hierarchy\"]))\n",
    "test = test.withColumn(\"Problem Unit\",get_unit(test[\"Problem Hierarchy\"]))\n",
    "test = test.withColumn(\"Problem Section\",get_section(test[\"Problem Hierarchy\"]))\n",
    "# train = train.drop('Problem Hierarchy')\n",
    "# test = test.drop('Problem Hierarchy')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get the number of KC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def KC_num(str):\n",
    "    if str:\n",
    "        return str.count(\"~~\")+1\n",
    "    else:\n",
    "        return 0\n",
    "train = train.withColumn(\"KC num\",KC_num(train['KC(Default)']))\n",
    "test = test.withColumn(\"KC num\",KC_num(test['KC(Default)']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get oportunity min and avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf \n",
    "def opp_avg(str):\n",
    "    if str:\n",
    "        str_list = str.split(\"~~\")\n",
    "        num_list = []\n",
    "        for s in str_list:\n",
    "            num = int(s)\n",
    "            num_list.append(num)\n",
    "        return 1.0 * sum(num_list)/len(num_list)\n",
    "    else:\n",
    "        return 0\n",
    "train = train.withColumn(\"Opportunity Average\",opp_avg(train['Opportunity(Default)']))\n",
    "test = test.withColumn(\"Opportunity Average\",opp_avg(test['Opportunity(Default)']))\n",
    "\n",
    "@udf \n",
    "def opp_min(str):\n",
    "    if str:\n",
    "        str_list = str.split(\"~~\")\n",
    "        num_list = []\n",
    "        for s in str_list:\n",
    "            num = int(s)\n",
    "            num_list.append(num)\n",
    "        return min(num_list)\n",
    "    else:\n",
    "        return 0\n",
    "train = train.withColumn(\"Opportunity Min\",opp_min(train[\"Opportunity(Default)\"]))\n",
    "test = test.withColumn(\"Opportunity Min\",opp_min(test[\"Opportunity(Default)\"]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Name Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def step_name_rename(str):\n",
    "    if str:\n",
    "        basic_op = [\"+\",\"-\",\"*\",\"/\",\"^\",\"=\"]\n",
    "        for op in basic_op:\n",
    "            if op in str:\n",
    "                return \"basic op\"\n",
    "        return str\n",
    "\n",
    "train = train.withColumn(\"renamed Step Name\",step_name_rename(train[\"Step Name\"]))\n",
    "train = train.drop(\"Step Name\")\n",
    "train = train.withColumnRenamed(\"renamed Step Name\",\"Step Name\")\n",
    "test = test.withColumn(\"renamed Step Name\",step_name_rename(test[\"Step Name\"]))\n",
    "test = test.drop(\"Step Name\")\n",
    "test = test.withColumnRenamed(\"renamed Step Name\",\"Step Name\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def encode(column):\n",
    "    global train,test\n",
    "    index = {}\n",
    "    all_rows = train.union(test).select(column).distinct().collect()\n",
    "    col_value = []\n",
    "    for item in all_rows:\n",
    "        col_value.append(item[column])\n",
    "    for i, val in enumerate(col_value):\n",
    "        index[val] = i\n",
    "    @udf\n",
    "    def get_id(str):\n",
    "        return index[str]\n",
    "    \n",
    "    encoded_column = \"encoded\" + \" \" + column \n",
    "    train = train.withColumn(encoded_column, get_id(train[column]))\n",
    "    train = train.drop(column)\n",
    "    train = train.withColumnRenamed(encoded_column, column)\n",
    "    test = test.withColumn(encoded_column, get_id(test[column]))\n",
    "    test = test.drop(column)\n",
    "    test = test.withColumnRenamed(encoded_column, column)\n",
    "\n",
    "col_encode = ['Anon Student Id','Problem Name','Problem Unit','Problem Section','Step Name']\n",
    "for col in col_encode:\n",
    "    encode(col)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "correct_train = train.filter(train[\"Correct First Attempt\"] == \"1\")\n",
    "def CFAR(column):\n",
    "    global train,test,correct_train\n",
    "    correct_group = correct_train.groupBy(column).count().collect()\n",
    "    all_group = train.groupBy(column).count().collect()\n",
    "    CFAR = {}\n",
    "    for col in all_group:\n",
    "        CFAR[col[column]] = [col['count'],0,0]\n",
    "    for col in correct_group:\n",
    "        CFAR[col[column]][1] = col['count']\n",
    "    for col in all_group:\n",
    "        CFAR[col[column]][2] = float(CFAR[col[column]][1] / CFAR[col[column]][0])\n",
    "    sum = 0\n",
    "    for col_value in CFAR:\n",
    "        sum += CFAR[col_value][2]\n",
    "    avg = float(sum / len(CFAR))\n",
    "    @udf\n",
    "    def get_rate(value):\n",
    "        if value in CFAR:\n",
    "            return CFAR[value][2]\n",
    "        else:\n",
    "            return avg\n",
    "    train = train.withColumn(column + \" CFAR\",get_rate(train[column]))\n",
    "    test = test.withColumn(column +\" CFAR\",get_rate(test[column]))\n",
    "\n",
    "CFAR_list = ['Anon Student Id','Problem Name','Problem Unit','Problem Section','Step Name','KC(Default)']\n",
    "for col in CFAR_list:\n",
    "    CFAR(col)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop useless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['Row','Step Start time','First Transaction Time','Correct Transaction Time','Step End Time','Step Duration (sec)',\\\n",
    "    'Correct Step Duration (sec)','Error Step Duration (sec)','Incorrects','Hints','Corrects','KC(Default)',\"Opportunity(Default)\",\"Problem Hierarchy\"]\n",
    "\n",
    "for col in columns_to_drop:\n",
    "    train =  train.drop(col)\n",
    "    test = test.drop(col)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train.toPandas().to_csv('data/train_pyspark.csv', sep='\\t', header=True, index = False)\n",
    "test.toPandas().to_csv('data/test_pyspark.csv', sep='\\t', header=True, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 08:50:36) \n[Clang 10.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dbe50fc7053e808575e65e0a42403fbc28ff2f2fdf42e40d83775c67c7cc7e02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
